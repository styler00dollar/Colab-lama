{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-lama.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s--ukxsdwzp6"
      },
      "source": [
        "# Colab-lama\n",
        "\n",
        "Original repo: [saic-mdal/lama](https://github.com/saic-mdal/lama)\n",
        "\n",
        "Original Colab: [here](https://colab.research.google.com/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb#scrollTo=ml8yanlBRJI4)\n",
        "\n",
        "My fork: [styler00dollar/Colab-lama](https://github.com/styler00dollar/Colab-lama)\n",
        "\n",
        "Mainly just less boxes, trying to have cleaner code and automatic cpu/gpu choise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UvV5nV7ww7_"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtHy8zzJw3X2",
        "cellView": "form"
      },
      "source": [
        "#@title install\n",
        "%cd /content/\n",
        "!git clone https://github.com/saic-mdal/lama\n",
        "%cd /content/lama\n",
        "!pip install -r requirements.txt \n",
        "!curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip\n",
        "!unzip big-lama.zip\n",
        "!mkdir /content/lama/data_for_prediction\n",
        "!pip install opencv-python-headless opencv-python --force-reinstall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fix files\n",
        "Only run this if you plan to draw. Uncomplete for batch inference. Batch will only work with original python files."
      ],
      "metadata": {
        "id": "THucGMk_Tl-9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fmo0xaViu32",
        "cellView": "form"
      },
      "source": [
        "#@title predict.py (fixing paths, removing hydra and try)\n",
        "%%writefile /content/lama/predict.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "from saicinpainting.evaluation.utils import move_to_device\n",
        "\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "\n",
        "import cv2\n",
        "import hydra\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from saicinpainting.training.data.datasets import make_default_val_dataset\n",
        "from saicinpainting.training.trainers import load_checkpoint\n",
        "from saicinpainting.utils import register_debug_signal_handlers\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "    register_debug_signal_handlers()  # kill -10 <pid> will result in traceback dumped into log\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_config_path = \"/content/lama/big-lama/config.yaml\"\n",
        "    with open(train_config_path, 'r') as f:\n",
        "        train_config = OmegaConf.create(yaml.safe_load(f))\n",
        "    \n",
        "    train_config.training_model.predict_only = True\n",
        "\n",
        "    out_ext = \".png\"\n",
        "\n",
        "    checkpoint_path = os.path.join(\"/content/lama/big-lama/models/best.ckpt\")\n",
        "    model = load_checkpoint(train_config, checkpoint_path, strict=False, map_location='cpu')\n",
        "    model.freeze()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      image = cv2.imread(\"/content/lama/data_for_prediction/image.png\")\n",
        "      image = torch.from_numpy(image).unsqueeze(0).permute(0,3,1,2)/255\n",
        "\n",
        "      mask = cv2.imread(\"/content/lama/png_mask.png\")\n",
        "      mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "      mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)/255\n",
        "\n",
        "      out = model(image, mask)\n",
        "      save_image(out[:, [2, 1, 0]], '/content/output.png')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xeIc0wH1BiON"
      },
      "source": [
        "#@title default.py (simple model(image, mask) input and single image return)\n",
        "%%writefile /content/lama/saicinpainting/training/trainers/default.py\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from saicinpainting.training.data.datasets import make_constant_area_crop_params\n",
        "from saicinpainting.training.losses.distance_weighting import make_mask_distance_weighter\n",
        "from saicinpainting.training.losses.feature_matching import feature_matching_loss, masked_l1_loss\n",
        "from saicinpainting.training.modules.fake_fakes import FakeFakesGenerator\n",
        "from saicinpainting.training.trainers.base import BaseInpaintingTrainingModule, make_multiscale_noise\n",
        "from saicinpainting.utils import add_prefix_to_keys, get_ramp\n",
        "from torchvision.utils import save_image\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def make_constant_area_crop_batch(batch, **kwargs):\n",
        "    crop_y, crop_x, crop_height, crop_width = make_constant_area_crop_params(img_height=batch['image'].shape[2],\n",
        "                                                                             img_width=batch['image'].shape[3],\n",
        "                                                                             **kwargs)\n",
        "    batch['image'] = batch['image'][:, :, crop_y : crop_y + crop_height, crop_x : crop_x + crop_width]\n",
        "    batch['mask'] = batch['mask'][:, :, crop_y: crop_y + crop_height, crop_x: crop_x + crop_width]\n",
        "    return batch\n",
        "\n",
        "\n",
        "class DefaultInpaintingTrainingModule(BaseInpaintingTrainingModule):\n",
        "    def __init__(self, *args, concat_mask=True, rescale_scheduler_kwargs=None, image_to_discriminator='predicted_image',\n",
        "                 add_noise_kwargs=None, noise_fill_hole=False, const_area_crop_kwargs=None,\n",
        "                 distance_weighter_kwargs=None, distance_weighted_mask_for_discr=False,\n",
        "                 fake_fakes_proba=0, fake_fakes_generator_kwargs=None,\n",
        "                 **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.concat_mask = concat_mask\n",
        "        self.rescale_size_getter = get_ramp(**rescale_scheduler_kwargs) if rescale_scheduler_kwargs is not None else None\n",
        "        self.image_to_discriminator = image_to_discriminator\n",
        "        self.add_noise_kwargs = add_noise_kwargs\n",
        "        self.noise_fill_hole = noise_fill_hole\n",
        "        self.const_area_crop_kwargs = const_area_crop_kwargs\n",
        "        self.refine_mask_for_losses = make_mask_distance_weighter(**distance_weighter_kwargs) \\\n",
        "            if distance_weighter_kwargs is not None else None\n",
        "        self.distance_weighted_mask_for_discr = distance_weighted_mask_for_discr\n",
        "\n",
        "        self.fake_fakes_proba = fake_fakes_proba\n",
        "        if self.fake_fakes_proba > 1e-3:\n",
        "            self.fake_fakes_gen = FakeFakesGenerator(**(fake_fakes_generator_kwargs or {}))\n",
        "\n",
        "    def forward(self, img, mask):\n",
        "        \"\"\"\n",
        "        if self.training and self.rescale_size_getter is not None:\n",
        "            cur_size = self.rescale_size_getter(self.global_step)\n",
        "            batch['image'] = F.interpolate(batch['image'], size=cur_size, mode='bilinear', align_corners=False)\n",
        "            batch['mask'] = F.interpolate(batch['mask'], size=cur_size, mode='nearest')\n",
        "\n",
        "        if self.training and self.const_area_crop_kwargs is not None:\n",
        "            batch = make_constant_area_crop_batch(batch, **self.const_area_crop_kwargs)\n",
        "        \"\"\"\n",
        "        #img = batch['image']\n",
        "        #mask = batch['mask']\n",
        "\n",
        "        masked_img = img * (1 - mask)\n",
        "\n",
        "        if self.add_noise_kwargs is not None:\n",
        "            noise = make_multiscale_noise(masked_img, **self.add_noise_kwargs)\n",
        "            if self.noise_fill_hole:\n",
        "                masked_img = masked_img + mask * noise[:, :masked_img.shape[1]]\n",
        "            masked_img = torch.cat([masked_img, noise], dim=1)\n",
        "\n",
        "        if self.concat_mask:\n",
        "            masked_img = torch.cat([masked_img, mask], dim=1)\n",
        "\n",
        "        #batch['predicted_image'] = self.generator(masked_img)\n",
        "        #batch['inpainted'] = mask * batch['predicted_image'] + (1 - mask) * batch['image']\n",
        "        \"\"\"\n",
        "        if self.fake_fakes_proba > 1e-3:\n",
        "            if self.training and torch.rand(1).item() < self.fake_fakes_proba:\n",
        "                batch['fake_fakes'], batch['fake_fakes_masks'] = self.fake_fakes_gen(img, mask)\n",
        "                batch['use_fake_fakes'] = True\n",
        "            else:\n",
        "                batch['fake_fakes'] = torch.zeros_like(img)\n",
        "                batch['fake_fakes_masks'] = torch.zeros_like(mask)\n",
        "                batch['use_fake_fakes'] = False\n",
        "\n",
        "        batch['mask_for_losses'] = self.refine_mask_for_losses(img, batch['predicted_image'], mask) \\\n",
        "            if self.refine_mask_for_losses is not None and self.training \\\n",
        "            else mask\n",
        "        \"\"\"\n",
        "        return self.generator(masked_img)\n",
        "\n",
        "    def generator_loss(self, batch):\n",
        "        img = batch['image']\n",
        "        predicted_img = batch[self.image_to_discriminator]\n",
        "        original_mask = batch['mask']\n",
        "        supervised_mask = batch['mask_for_losses']\n",
        "\n",
        "        # L1\n",
        "        l1_value = masked_l1_loss(predicted_img, img, supervised_mask,\n",
        "                                  self.config.losses.l1.weight_known,\n",
        "                                  self.config.losses.l1.weight_missing)\n",
        "\n",
        "        total_loss = l1_value\n",
        "        metrics = dict(gen_l1=l1_value)\n",
        "\n",
        "        # vgg-based perceptual loss\n",
        "        if self.config.losses.perceptual.weight > 0:\n",
        "            pl_value = self.loss_pl(predicted_img, img, mask=supervised_mask).sum() * self.config.losses.perceptual.weight\n",
        "            total_loss = total_loss + pl_value\n",
        "            metrics['gen_pl'] = pl_value\n",
        "\n",
        "        # discriminator\n",
        "        # adversarial_loss calls backward by itself\n",
        "        mask_for_discr = supervised_mask if self.distance_weighted_mask_for_discr else original_mask\n",
        "        self.adversarial_loss.pre_generator_step(real_batch=img, fake_batch=predicted_img,\n",
        "                                                 generator=self.generator, discriminator=self.discriminator)\n",
        "        discr_real_pred, discr_real_features = self.discriminator(img)\n",
        "        discr_fake_pred, discr_fake_features = self.discriminator(predicted_img)\n",
        "        adv_gen_loss, adv_metrics = self.adversarial_loss.generator_loss(real_batch=img,\n",
        "                                                                         fake_batch=predicted_img,\n",
        "                                                                         discr_real_pred=discr_real_pred,\n",
        "                                                                         discr_fake_pred=discr_fake_pred,\n",
        "                                                                         mask=mask_for_discr)\n",
        "        total_loss = total_loss + adv_gen_loss\n",
        "        metrics['gen_adv'] = adv_gen_loss\n",
        "        metrics.update(add_prefix_to_keys(adv_metrics, 'adv_'))\n",
        "\n",
        "        # feature matching\n",
        "        if self.config.losses.feature_matching.weight > 0:\n",
        "            need_mask_in_fm = OmegaConf.to_container(self.config.losses.feature_matching).get('pass_mask', False)\n",
        "            mask_for_fm = supervised_mask if need_mask_in_fm else None\n",
        "            fm_value = feature_matching_loss(discr_fake_features, discr_real_features,\n",
        "                                             mask=mask_for_fm) * self.config.losses.feature_matching.weight\n",
        "            total_loss = total_loss + fm_value\n",
        "            metrics['gen_fm'] = fm_value\n",
        "\n",
        "        if self.loss_resnet_pl is not None:\n",
        "            resnet_pl_value = self.loss_resnet_pl(predicted_img, img)\n",
        "            total_loss = total_loss + resnet_pl_value\n",
        "            metrics['gen_resnet_pl'] = resnet_pl_value\n",
        "\n",
        "        return total_loss, metrics\n",
        "\n",
        "    def discriminator_loss(self, batch):\n",
        "        total_loss = 0\n",
        "        metrics = {}\n",
        "\n",
        "        predicted_img = batch[self.image_to_discriminator].detach()\n",
        "        self.adversarial_loss.pre_discriminator_step(real_batch=batch['image'], fake_batch=predicted_img,\n",
        "                                                     generator=self.generator, discriminator=self.discriminator)\n",
        "        discr_real_pred, discr_real_features = self.discriminator(batch['image'])\n",
        "        discr_fake_pred, discr_fake_features = self.discriminator(predicted_img)\n",
        "        adv_discr_loss, adv_metrics = self.adversarial_loss.discriminator_loss(real_batch=batch['image'],\n",
        "                                                                               fake_batch=predicted_img,\n",
        "                                                                               discr_real_pred=discr_real_pred,\n",
        "                                                                               discr_fake_pred=discr_fake_pred,\n",
        "                                                                               mask=batch['mask'])\n",
        "        total_loss = total_loss + adv_discr_loss\n",
        "        metrics['discr_adv'] = adv_discr_loss\n",
        "        metrics.update(add_prefix_to_keys(adv_metrics, 'adv_'))\n",
        "\n",
        "\n",
        "        if batch.get('use_fake_fakes', False):\n",
        "            fake_fakes = batch['fake_fakes']\n",
        "            self.adversarial_loss.pre_discriminator_step(real_batch=batch['image'], fake_batch=fake_fakes,\n",
        "                                                         generator=self.generator, discriminator=self.discriminator)\n",
        "            discr_fake_fakes_pred, _ = self.discriminator(fake_fakes)\n",
        "            fake_fakes_adv_discr_loss, fake_fakes_adv_metrics = self.adversarial_loss.discriminator_loss(\n",
        "                real_batch=batch['image'],\n",
        "                fake_batch=fake_fakes,\n",
        "                discr_real_pred=discr_real_pred,\n",
        "                discr_fake_pred=discr_fake_fakes_pred,\n",
        "                mask=batch['mask']\n",
        "            )\n",
        "            total_loss = total_loss + fake_fakes_adv_discr_loss\n",
        "            metrics['discr_adv_fake_fakes'] = fake_fakes_adv_discr_loss\n",
        "            metrics.update(add_prefix_to_keys(fake_fakes_adv_metrics, 'adv_'))\n",
        "\n",
        "        return total_loss, metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draw"
      ],
      "metadata": {
        "id": "QVb_rrIoTkr3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vJ1whuM7hY40"
      },
      "source": [
        "\n",
        "#@title Drawing code\n",
        "import base64\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "<button>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNANv00axRpw",
        "cellView": "form"
      },
      "source": [
        "#@title upload image\n",
        "from google.colab import files\n",
        "files = files.upload()\n",
        "fname = list(files.keys())[0]\n",
        "! rm -r data_for_prediction\n",
        "! mkdir data_for_prediction\n",
        "\n",
        "#copyfile(fname, f'/content/lama/data_for_prediction/{fname}')\n",
        "#fname = f'/content/lama/data_for_prediction/{fname}'\n",
        "\n",
        "copyfile(fname, f'/content/lama/data_for_prediction/image.png')\n",
        "fname = f'/content/lama/data_for_prediction/image.png'\n",
        "\n",
        "image64 = base64.b64encode(open(fname, 'rb').read())\n",
        "image64 = image64.decode('utf-8')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "print(f'Will use {fname} for inpainting')\n",
        "img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
        "# _ = plt.imshow(img)\n",
        "# _ = plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WznzcTtHxWW1",
        "cellView": "form"
      },
      "source": [
        "#@title Draw a mask and press Finish \n",
        "%cd /content/lama\n",
        "draw(image64, filename=f\"./{fname.split('.')[1]}_mask.png\", w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "\n",
        "plt.subplot(131)\n",
        "with_mask = np.array(plt.imread(f\"./{fname.split('.')[1]}_mask.png\")[:,:,:3])\n",
        "mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.title('mask')\n",
        "plt.imsave(f\"./{fname.split('.')[1]}_mask.png\",mask, cmap='gray')\n",
        "\n",
        "plt.subplot(132)\n",
        "img = np.array(plt.imread(f'{fname}')[:,:,:3])\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title('img')\n",
        "\n",
        "plt.subplot(133)\n",
        "img = np.array((1-mask.reshape(mask.shape[0], mask.shape[1], -1))*plt.imread(fname)[:,:,:3])\n",
        "_=plt.imshow(img)\n",
        "_=plt.axis('off')\n",
        "_=plt.title('img * mask')\n",
        "\n",
        "#!PYTHONPATH=. python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/output dataset.img_suffix=.png > /dev/null\n",
        "!python3 predict.py model.path=/content/lama/big-lama \\\n",
        " indir=/content/lama/data_for_prediction outdir=/content/lama/outputs dataset.img_suffix=.png > /dev/null\n",
        "\n",
        "plt.imshow(plt.imread(f\"/content/output.png\"))\n",
        "_=plt.axis('off')\n",
        "_=plt.title('inpainting result')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch"
      ],
      "metadata": {
        "id": "EPWtPqFOTo_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch inference\n",
        "#@markdown Place files into `/content/lama/data_for_prediction`. The images need to be marked with green. Outputs are in `/content/lama/outputs`.\n",
        "%cd /content/lama\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "rootdir = \"/content/lama/data_for_prediction/\"\n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "\n",
        "for f in tqdm(files):\n",
        "  image = cv2.imread(f)\n",
        "  mask = np.all(image == [0,255,0], axis=-1)\n",
        "  image[mask]=0\n",
        "\n",
        "  os.remove(f)\n",
        "  cv2.imwrite(os.path.join(rootdir, os.path.splitext(f)[0]+\".png\"), image)\n",
        "  cv2.imwrite(os.path.join(rootdir, os.path.splitext(f)[0]+\"_mask.png\"), np.array(mask, dtype=np.uint8)*255)\n",
        "!PYTHONPATH=. TORCH_HOME=$(pwd) python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/data_for_prediction outdir=/content/lama/outputs  dataset.img_suffix=.png > /dev/null"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6o5rkuzNS7mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title delete inputs/outputs\n",
        "%cd /content\n",
        "!sudo rm -rf /content/lama/data_for_prediction\n",
        "!mkdir /content/lama/data_for_prediction\n",
        "!sudo rm -rf /content/lama/outputs\n",
        "!mkdir /content/lama/outputs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3ZPaXE4OVisR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
